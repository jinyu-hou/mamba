++ date +%s
+ timestamp=1711335574
+ result_dir=evals/results
+ model_id=mamba-2.8b-slimpj
++ echo mamba-2.8b-slimpj
++ tr - _
+ result_prefix=mamba_2.8b_slimpj
+ preserve_rate=0.3
++ awk -vn=0.3 'BEGIN{printf("%.0f\n",n*100)}'
+ preserve_percentage=30
+ rm evals/results/mamba_2.8b_slimpj-30.jsonl
rm: cannot remove 'evals/results/mamba_2.8b_slimpj-30.jsonl': No such file or directory
+ accelerate launch evals/lm_harness_eval.py --model mamba --model_args pretrained=state-spaces/mamba-2.8b-slimpj,preserve_rate=0.3 --tasks boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa,race,truthfulqa_mc2 --device cuda --batch_size 16 --output_path evals/results/mamba_2.8b_slimpj-30.jsonl
2024-03-24:22:03:27,136 INFO     [utils.py:148] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-24:22:03:27,136 INFO     [utils.py:148] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-24:22:03:27,136 INFO     [utils.py:148] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-24:22:03:27,137 INFO     [utils.py:148] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-24:22:03:36,063 INFO     [config.py:58] PyTorch version 2.2.1+cu118 available.
2024-03-24:22:03:36,073 INFO     [config.py:58] PyTorch version 2.2.1+cu118 available.
2024-03-24:22:03:36,073 INFO     [config.py:58] PyTorch version 2.2.1+cu118 available.
2024-03-24:22:03:36,073 INFO     [config.py:58] PyTorch version 2.2.1+cu118 available.
2024-03-24:22:04:55,918 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for gsmk boolq, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:22:04:55,918 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for gsmk boolq, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:22:04:55,918 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for gsmk boolq, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:22:04:55,918 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for gsmk boolq, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:22:04:55,920 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for EleutherAI/asdiv, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:22:04:55,920 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for EleutherAI/asdiv, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:22:04:55,920 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for EleutherAI/asdiv, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:22:04:55,920 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for EleutherAI/asdiv, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:22:04:58,601 INFO     [__main__.py:184] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'race', 'truthfulqa_mc2', 'winogrande']
2024-03-24:22:04:58,601 INFO     [__main__.py:184] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'race', 'truthfulqa_mc2', 'winogrande']
2024-03-24:22:04:58,601 INFO     [__main__.py:184] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'race', 'truthfulqa_mc2', 'winogrande']
2024-03-24:22:04:58,601 INFO     [__main__.py:184] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'race', 'truthfulqa_mc2', 'winogrande']
Traceback (most recent call last):
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 76, in <module>
    cli_evaluate()
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/__main__.py", line 186, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/utils.py", line 343, in _wrapper
    return fn(*args, **kwargs)
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/evaluator.py", line 90, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/api/model.py", line 140, in create_from_arg_string
    return cls(**args, **args2)
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 29, in __init__
    self._model = MambaLMHeadModel.from_pretrained(pretrained, dtype=dtype, device=device)
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 251, in from_pretrained
    model = cls(config, device=device, dtype=dtype, **kwargs)
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 214, in __init__
    self.lm_head = nn.Linear(d_model, vocab_size, bias=False, **factory_kwargs)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 98, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 492.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 287.44 MiB is free. Including non-PyTorch memory, this process has 10.65 GiB memory in use. Process 3240159 has 11.13 GiB memory in use. Process 3240158 has 11.13 GiB memory in use. Process 3240157 has 11.13 GiB memory in use. Of the allocated memory 10.31 GiB is allocated by PyTorch, and 40.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 76, in <module>
    cli_evaluate()
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/__main__.py", line 186, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/utils.py", line 343, in _wrapper
    return fn(*args, **kwargs)
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/evaluator.py", line 90, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/api/model.py", line 140, in create_from_arg_string
    return cls(**args, **args2)
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 29, in __init__
    self._model = MambaLMHeadModel.from_pretrained(pretrained, dtype=dtype, device=device)
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 252, in from_pretrained
    model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))
  File "/projects/bcmr/hou1/mamba/mamba_ssm/utils/hf.py", line 18, in load_state_dict_hf
    return torch.load(resolved_archive_file, map_location=mapped_device)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1026, in load
    return _load(opened_zipfile,
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1438, in _load
    result = unpickler.load()
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1408, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1382, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1308, in restore_location
    return default_restore_location(storage, map_location)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 391, in default_restore_location
    result = fn(storage, location)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 271, in _cuda_deserialize
    return obj.cuda(device)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/_utils.py", line 115, in _cuda
    untyped_storage = torch.UntypedStorage(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 87.44 MiB is free. Process 3240156 has 10.65 GiB memory in use. Including non-PyTorch memory, this process has 11.13 GiB memory in use. Process 3240158 has 11.23 GiB memory in use. Process 3240157 has 11.23 GiB memory in use. Of the allocated memory 10.79 GiB is allocated by PyTorch, and 40.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 76, in <module>
    cli_evaluate()
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/__main__.py", line 186, in cli_evaluate
Traceback (most recent call last):
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 76, in <module>
    results = evaluator.simple_evaluate(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/utils.py", line 343, in _wrapper
    cli_evaluate()
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/__main__.py", line 186, in cli_evaluate
    return fn(*args, **kwargs)
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/evaluator.py", line 90, in simple_evaluate
    results = evaluator.simple_evaluate(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/utils.py", line 343, in _wrapper
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/api/model.py", line 140, in create_from_arg_string
    return fn(*args, **kwargs)
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/evaluator.py", line 90, in simple_evaluate
    return cls(**args, **args2)
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 29, in __init__
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/api/model.py", line 140, in create_from_arg_string
    self._model = MambaLMHeadModel.from_pretrained(pretrained, dtype=dtype, device=device)
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 252, in from_pretrained
    return cls(**args, **args2)
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 29, in __init__
    self._model = MambaLMHeadModel.from_pretrained(pretrained, dtype=dtype, device=device)    
model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 252, in from_pretrained

  File "/projects/bcmr/hou1/mamba/mamba_ssm/utils/hf.py", line 18, in load_state_dict_hf
    return torch.load(resolved_archive_file, map_location=mapped_device)
      File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1026, in load
model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))
  File "/projects/bcmr/hou1/mamba/mamba_ssm/utils/hf.py", line 18, in load_state_dict_hf
    return torch.load(resolved_archive_file, map_location=mapped_device)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1026, in load
    return _load(opened_zipfile,
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1438, in _load
    return _load(opened_zipfile,
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1438, in _load
    result = unpickler.load()
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1408, in persistent_load
    result = unpickler.load()
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1408, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1382, in load_tensor
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1382, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1308, in restore_location
    wrap_storage=restore_location(storage, location),
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1308, in restore_location
    return default_restore_location(storage, map_location)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 391, in default_restore_location
    return default_restore_location(storage, map_location)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 391, in default_restore_location
    result = fn(storage, location)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 271, in _cuda_deserialize
    result = fn(storage, location)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 271, in _cuda_deserialize
    return obj.cuda(device)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/_utils.py", line 115, in _cuda
    return obj.cuda(device)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/_utils.py", line 115, in _cuda
    untyped_storage = torch.UntypedStorage(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 47.44 MiB is free. Process 3240156 has 10.65 GiB memory in use. Process 3240159 has 11.13 GiB memory in use. Process 3240158 has 11.25 GiB memory in use. Including non-PyTorch memory, this process has 11.25 GiB memory in use. Of the allocated memory 10.90 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    untyped_storage = torch.UntypedStorage(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 47.44 MiB is free. Process 3240156 has 10.65 GiB memory in use. Process 3240159 has 11.13 GiB memory in use. Including non-PyTorch memory, this process has 11.25 GiB memory in use. Process 3240157 has 11.25 GiB memory in use. Of the allocated memory 10.90 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-03-24 22:05:12,142] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3240156) of binary: /projects/bcmr/hou1/envs/fiona/bin/python
Traceback (most recent call last):
  File "/projects/bcmr/hou1/envs/fiona/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1010, in launch_command
    multi_gpu_launcher(args)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/accelerate/commands/launch.py", line 672, in multi_gpu_launcher
    distrib_run.run(args)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
evals/lm_harness_eval.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-03-24_22:05:12
  host      : gpub070.delta.ncsa.illinois.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3240157)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-03-24_22:05:12
  host      : gpub070.delta.ncsa.illinois.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3240158)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-03-24_22:05:12
  host      : gpub070.delta.ncsa.illinois.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3240159)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-24_22:05:12
  host      : gpub070.delta.ncsa.illinois.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3240156)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
