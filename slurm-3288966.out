++ date +%s
+ timestamp=1711340513
+ result_dir=evals/results
+ model_id=mamba-2.8b-slimpj
++ echo mamba-2.8b-slimpj
++ tr - _
+ result_prefix=mamba_2.8b_slimpj
+ preserve_rate=0.25
++ awk -vn=0.25 'BEGIN{printf("%.0f\n",n*100)}'
+ preserve_percentage=25
+ rm evals/results/mamba_2.8b_slimpj-25.jsonl
+ accelerate launch evals/lm_harness_eval.py --model mamba --model_args pretrained=state-spaces/mamba-2.8b-slimpj,preserve_rate=0.25 --tasks boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa,race,truthfulqa_mc2 --device cuda --batch_size 16 --output_path evals/results/mamba_2.8b_slimpj-25.jsonl
2024-03-24:23:26:08,997 INFO     [utils.py:148] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-24:23:26:08,997 INFO     [utils.py:148] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-24:23:26:08,997 INFO     [utils.py:148] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-24:23:26:08,997 INFO     [utils.py:148] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-24:23:26:15,384 INFO     [config.py:58] PyTorch version 2.2.1+cu118 available.
2024-03-24:23:26:15,384 INFO     [config.py:58] PyTorch version 2.2.1+cu118 available.
2024-03-24:23:26:15,384 INFO     [config.py:58] PyTorch version 2.2.1+cu118 available.
2024-03-24:23:26:15,384 INFO     [config.py:58] PyTorch version 2.2.1+cu118 available.
2024-03-24:23:27:30,589 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for gsmk boolq, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:23:27:30,589 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for gsmk boolq, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:23:27:30,591 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for EleutherAI/asdiv, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:23:27:30,591 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for EleutherAI/asdiv, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:23:27:33,388 INFO     [__main__.py:184] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'race', 'truthfulqa_mc2', 'winogrande']
2024-03-24:23:27:33,388 INFO     [__main__.py:184] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'race', 'truthfulqa_mc2', 'winogrande']
2024-03-24:23:27:45,605 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for gsmk boolq, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:23:27:45,607 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for EleutherAI/asdiv, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:23:27:45,633 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for gsmk boolq, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:23:27:45,636 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for EleutherAI/asdiv, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:23:27:46,924 INFO     [__main__.py:184] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'race', 'truthfulqa_mc2', 'winogrande']
2024-03-24:23:27:46,954 INFO     [__main__.py:184] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'race', 'truthfulqa_mc2', 'winogrande']
Traceback (most recent call last):
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 76, in <module>
Traceback (most recent call last):
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 76, in <module>
    cli_evaluate()
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/__main__.py", line 186, in cli_evaluate
    cli_evaluate()
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/__main__.py", line 186, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/utils.py", line 343, in _wrapper
    results = evaluator.simple_evaluate(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/utils.py", line 343, in _wrapper
    return fn(*args, **kwargs)
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/evaluator.py", line 90, in simple_evaluate
    return fn(*args, **kwargs)
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/evaluator.py", line 90, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/api/model.py", line 140, in create_from_arg_string
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/api/model.py", line 140, in create_from_arg_string
    return cls(**args, **args2)
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 29, in __init__
    return cls(**args, **args2)
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 29, in __init__
    self._model = MambaLMHeadModel.from_pretrained(pretrained, dtype=dtype, device=device)
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 252, in from_pretrained
    self._model = MambaLMHeadModel.from_pretrained(pretrained, dtype=dtype, device=device)
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 252, in from_pretrained
    model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))
  File "/projects/bcmr/hou1/mamba/mamba_ssm/utils/hf.py", line 18, in load_state_dict_hf
    model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))
  File "/projects/bcmr/hou1/mamba/mamba_ssm/utils/hf.py", line 18, in load_state_dict_hf
    return torch.load(resolved_archive_file, map_location=mapped_device)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1026, in load
    return torch.load(resolved_archive_file, map_location=mapped_device)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1026, in load
    return _load(opened_zipfile,
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1438, in _load
    return _load(opened_zipfile,
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1438, in _load
    result = unpickler.load()
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1408, in persistent_load
    result = unpickler.load()
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1408, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1382, in load_tensor
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1382, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1308, in restore_location
    wrap_storage=restore_location(storage, location),
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1308, in restore_location
    return default_restore_location(storage, map_location)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 391, in default_restore_location
    return default_restore_location(storage, map_location)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 391, in default_restore_location
    result = fn(storage, location)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 271, in _cuda_deserialize
    result = fn(storage, location)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 271, in _cuda_deserialize
    return obj.cuda(device)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/_utils.py", line 115, in _cuda
    return obj.cuda(device)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/_utils.py", line 115, in _cuda
    untyped_storage = torch.UntypedStorage(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 33.50 MiB is free. Including non-PyTorch memory, this process has 13.60 GiB memory in use. Process 761808 has 13.55 GiB memory in use. Process 761806 has 9.42 GiB memory in use. Process 761805 has 7.72 GiB memory in use. Of the allocated memory 13.25 GiB is allocated by PyTorch, and 51.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    untyped_storage = torch.UntypedStorage(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 33.50 MiB is free. Process 761807 has 13.60 GiB memory in use. Including non-PyTorch memory, this process has 13.55 GiB memory in use. Process 761806 has 9.42 GiB memory in use. Process 761805 has 7.72 GiB memory in use. Of the allocated memory 13.20 GiB is allocated by PyTorch, and 51.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 76, in <module>
Traceback (most recent call last):
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 76, in <module>
    cli_evaluate()
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/__main__.py", line 186, in cli_evaluate
    cli_evaluate()
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/__main__.py", line 186, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/utils.py", line 343, in _wrapper
    results = evaluator.simple_evaluate(    
return fn(*args, **kwargs)  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/utils.py", line 343, in _wrapper

  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/evaluator.py", line 90, in simple_evaluate
        return fn(*args, **kwargs)lm = lm_eval.api.registry.get_model(model).create_from_arg_string(

  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/evaluator.py", line 90, in simple_evaluate
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/api/model.py", line 140, in create_from_arg_string
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(    
return cls(**args, **args2)  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/api/model.py", line 140, in create_from_arg_string

  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 29, in __init__
    return cls(**args, **args2)    
self._model = MambaLMHeadModel.from_pretrained(pretrained, dtype=dtype, device=device)  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 29, in __init__

  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 251, in from_pretrained
        self._model = MambaLMHeadModel.from_pretrained(pretrained, dtype=dtype, device=device)model = cls(config, device=device, dtype=dtype, **kwargs)

  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 251, in from_pretrained
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 203, in __init__
    self.backbone = MixerModel(
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 118, in __init__
    model = cls(config, device=device, dtype=dtype, **kwargs)
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 203, in __init__
    [
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 119, in <listcomp>
    self.backbone = MixerModel(
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 118, in __init__
    create_block(
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 42, in create_block
    [
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 119, in <listcomp>
    block = Block(
  File "/projects/bcmr/hou1/mamba/mamba_ssm/modules/mamba_lowrank.py", line 398, in __init__
    create_block(
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 42, in create_block
    block = Block(
  File "/projects/bcmr/hou1/mamba/mamba_ssm/modules/mamba_lowrank.py", line 398, in __init__
    self.mixer = mixer_cls(dim)
  File "/projects/bcmr/hou1/mamba/mamba_ssm/modules/mamba_lowrank.py", line 64, in __init__
    self.mixer = mixer_cls(dim)
  File "/projects/bcmr/hou1/mamba/mamba_ssm/modules/mamba_lowrank.py", line 64, in __init__
    self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 98, in __init__
    self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 98, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 83.50 MiB is free. Process 761807 has 13.55 GiB memory in use. Process 761808 has 13.55 GiB memory in use. Including non-PyTorch memory, this process has 9.42 GiB memory in use. Process 761805 has 7.72 GiB memory in use. Of the allocated memory 9.08 GiB is allocated by PyTorch, and 37.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 83.50 MiB is free. Process 761807 has 13.55 GiB memory in use. Process 761808 has 13.55 GiB memory in use. Process 761806 has 9.42 GiB memory in use. Including non-PyTorch memory, this process has 7.72 GiB memory in use. Of the allocated memory 7.39 GiB is allocated by PyTorch, and 31.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-03-24 23:28:01,663] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 761805) of binary: /projects/bcmr/hou1/envs/fiona/bin/python
Traceback (most recent call last):
  File "/projects/bcmr/hou1/envs/fiona/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1010, in launch_command
    multi_gpu_launcher(args)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/accelerate/commands/launch.py", line 672, in multi_gpu_launcher
    distrib_run.run(args)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
evals/lm_harness_eval.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-03-24_23:28:01
  host      : gpub063.delta.ncsa.illinois.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 761806)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-03-24_23:28:01
  host      : gpub063.delta.ncsa.illinois.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 761807)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-03-24_23:28:01
  host      : gpub063.delta.ncsa.illinois.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 761808)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-24_23:28:01
  host      : gpub063.delta.ncsa.illinois.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 761805)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
