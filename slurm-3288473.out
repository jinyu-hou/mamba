++ date +%s
+ timestamp=1711327502
+ result_dir=evals/results
+ model_id=mamba-2.8b-slimpj
++ echo mamba-2.8b-slimpj
++ tr - _
+ result_prefix=mamba_2.8b_slimpj
+ preserve_rate=0.5
++ awk -vn=0.5 'BEGIN{printf("%.0f\n",n*100)}'
+ preserve_percentage=50
+ rm evals/results/mamba_2.8b_slimpj-50.jsonl
+ accelerate launch evals/lm_harness_eval.py --model mamba --model_args pretrained=state-spaces/mamba-2.8b-slimpj,preserve_rate=0.5 --tasks boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa,race,truthfulqa_mc2 --device cuda --batch_size 16 --output_path evals/results/mamba_2.8b_slimpj-50.jsonl
2024-03-24:19:56:49,545 INFO     [utils.py:148] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-24:19:56:49,545 INFO     [utils.py:148] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-24:19:56:49,545 INFO     [utils.py:148] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-24:19:56:49,548 INFO     [utils.py:148] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-24:19:57:11,850 INFO     [config.py:58] PyTorch version 2.2.1+cu118 available.
2024-03-24:19:57:11,885 INFO     [config.py:58] PyTorch version 2.2.1+cu118 available.
2024-03-24:19:57:12,338 INFO     [config.py:58] PyTorch version 2.2.1+cu118 available.
2024-03-24:19:57:44,670 INFO     [config.py:58] PyTorch version 2.2.1+cu118 available.
2024-03-24:20:16:42,300 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for gsmk boolq, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:20:16:42,318 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for EleutherAI/asdiv, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:20:16:47,800 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for gsmk boolq, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:20:16:47,804 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for EleutherAI/asdiv, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:20:16:52,628 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for gsmk boolq, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:20:16:52,682 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for EleutherAI/asdiv, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:20:18:11,352 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for gsmk boolq, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:20:18:11,361 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for EleutherAI/asdiv, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.
2024-03-24:20:18:30,284 INFO     [__main__.py:184] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'race', 'truthfulqa_mc2', 'winogrande']
2024-03-24:20:18:30,289 INFO     [__main__.py:184] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'race', 'truthfulqa_mc2', 'winogrande']
2024-03-24:20:18:32,766 INFO     [__main__.py:184] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'race', 'truthfulqa_mc2', 'winogrande']
Traceback (most recent call last):
Traceback (most recent call last):
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 76, in <module>
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 76, in <module>
Traceback (most recent call last):
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 76, in <module>
    cli_evaluate()
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/__main__.py", line 186, in cli_evaluate
    cli_evaluate()
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/__main__.py", line 186, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/utils.py", line 343, in _wrapper
    return fn(*args, **kwargs)
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/evaluator.py", line 90, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/api/model.py", line 140, in create_from_arg_string
    cli_evaluate()
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/__main__.py", line 186, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/utils.py", line 343, in _wrapper
    return cls(**args, **args2)
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 29, in __init__
    return fn(*args, **kwargs)
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/evaluator.py", line 90, in simple_evaluate
    self._model = MambaLMHeadModel.from_pretrained(pretrained, dtype=dtype, device=device)
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 252, in from_pretrained
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/api/model.py", line 140, in create_from_arg_string
    return cls(**args, **args2)
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 29, in __init__
    model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))
  File "/projects/bcmr/hou1/mamba/mamba_ssm/utils/hf.py", line 18, in load_state_dict_hf
    self._model = MambaLMHeadModel.from_pretrained(pretrained, dtype=dtype, device=device)
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 252, in from_pretrained
    model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))
  File "/projects/bcmr/hou1/mamba/mamba_ssm/utils/hf.py", line 18, in load_state_dict_hf
    return torch.load(resolved_archive_file, map_location=mapped_device)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1026, in load
    return torch.load(resolved_archive_file, map_location=mapped_device)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1026, in load
    return _load(opened_zipfile,
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1438, in _load
    return _load(opened_zipfile,
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1438, in _load
    result = unpickler.load()
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1408, in persistent_load
    result = unpickler.load()
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1408, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1382, in load_tensor
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1382, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1308, in restore_location
    wrap_storage=restore_location(storage, location),
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1308, in restore_location
    return default_restore_location(storage, map_location)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 391, in default_restore_location
    return default_restore_location(storage, map_location)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 391, in default_restore_location
    result = fn(storage, location)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 271, in _cuda_deserialize
        result = fn(storage, location)return obj.cuda(device)

  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 271, in _cuda_deserialize
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/_utils.py", line 115, in _cuda
    return obj.cuda(device)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/_utils.py", line 115, in _cuda
    untyped_storage = torch.UntypedStorage(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 22.81 MiB is free. Including non-PyTorch memory, this process has 14.83 GiB memory in use. Process 3202128 has 14.69 GiB memory in use. Process 3202127 has 14.79 GiB memory in use. Of the allocated memory 14.48 GiB is allocated by PyTorch, and 56.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    untyped_storage = torch.UntypedStorage(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 72.81 MiB is free. Process 3202126 has 14.79 GiB memory in use. Including non-PyTorch memory, this process has 14.69 GiB memory in use. Process 3202127 has 14.79 GiB memory in use. Of the allocated memory 14.33 GiB is allocated by PyTorch, and 63.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    results = evaluator.simple_evaluate(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/utils.py", line 343, in _wrapper
    return fn(*args, **kwargs)
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/evaluator.py", line 90, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/projects/bcmr/hou1/mamba/3rdparty/lm-evaluation-harness/lm_eval/api/model.py", line 140, in create_from_arg_string
    return cls(**args, **args2)
  File "/projects/bcmr/hou1/mamba/evals/lm_harness_eval.py", line 29, in __init__
    self._model = MambaLMHeadModel.from_pretrained(pretrained, dtype=dtype, device=device)
  File "/projects/bcmr/hou1/mamba/mamba_ssm/models/mixer_seq_lowrank.py", line 252, in from_pretrained
    model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))
  File "/projects/bcmr/hou1/mamba/mamba_ssm/utils/hf.py", line 18, in load_state_dict_hf
    return torch.load(resolved_archive_file, map_location=mapped_device)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1026, in load
    return _load(opened_zipfile,
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1438, in _load
    result = unpickler.load()
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1408, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1382, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 1308, in restore_location
    return default_restore_location(storage, map_location)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 391, in default_restore_location
    result = fn(storage, location)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/serialization.py", line 271, in _cuda_deserialize
    return obj.cuda(device)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/_utils.py", line 115, in _cuda
    untyped_storage = torch.UntypedStorage(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 22.81 MiB is free. Process 3202126 has 14.83 GiB memory in use. Process 3202128 has 14.69 GiB memory in use. Including non-PyTorch memory, this process has 14.79 GiB memory in use. Of the allocated memory 14.43 GiB is allocated by PyTorch, and 56.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2024-03-24:20:19:17,165 INFO     [__main__.py:184] Selected Tasks: ['arc_challenge', 'arc_easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'race', 'truthfulqa_mc2', 'winogrande']
[2024-03-24 20:20:16,777] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3202129 closing signal SIGTERM
[2024-03-24 20:22:15,951] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3202126) of binary: /projects/bcmr/hou1/envs/fiona/bin/python
Traceback (most recent call last):
  File "/projects/bcmr/hou1/envs/fiona/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1010, in launch_command
    multi_gpu_launcher(args)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/accelerate/commands/launch.py", line 672, in multi_gpu_launcher
    distrib_run.run(args)
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/projects/bcmr/hou1/envs/fiona/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
evals/lm_harness_eval.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-03-24_20:20:16
  host      : gpub093.delta.ncsa.illinois.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3202127)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-03-24_20:20:16
  host      : gpub093.delta.ncsa.illinois.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3202128)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-24_20:20:16
  host      : gpub093.delta.ncsa.illinois.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3202126)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
