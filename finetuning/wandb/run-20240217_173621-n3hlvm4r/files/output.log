[rank: 0] Seed set to 11111
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/ocean/projects/cis240002p/jhou/envs/mamba/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
Traceback (most recent call last):
  File "/ocean/projects/cis240002p/jhou/mamba/finetuning/main.py", line 251, in <module>
    fire.Fire(main)
  File "/ocean/projects/cis240002p/jhou/envs/mamba/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/ocean/projects/cis240002p/jhou/envs/mamba/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/ocean/projects/cis240002p/jhou/envs/mamba/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/ocean/projects/cis240002p/jhou/mamba/finetuning/main.py", line 215, in main
    train_chunk(
  File "/ocean/projects/cis240002p/jhou/mamba/finetuning/main.py", line 107, in train_chunk
    logits = model(input_ids)[0]
  File "/ocean/projects/cis240002p/jhou/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ocean/projects/cis240002p/jhou/envs/mamba/lib/python3.10/site-packages/lightning/fabric/wrappers.py", line 138, in forward
    with self._precision.forward_context():
  File "/ocean/projects/cis240002p/jhou/envs/mamba/lib/python3.10/site-packages/lightning/fabric/plugins/precision/fsdp.py", line 117, in forward_context
    return torch.autocast("cuda", dtype=(torch.bfloat16 if self.precision == "bf16-mixed" else torch.float16))
  File "/ocean/projects/cis240002p/jhou/envs/mamba/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 225, in __init__
    raise RuntimeError('Current CUDA Device does not support bfloat16. Please switch dtype to float16.')
RuntimeError: Current CUDA Device does not support bfloat16. Please switch dtype to float16.